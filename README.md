
# **Big Data Processing with Docker**  

## **Overview**  
This project sets up a **scalable big data environment** using **Docker** and **Docker Compose** to facilitate **distributed data storage and processing**. It integrates **Hadoop HDFS, Apache Spark, and Jupyter Notebooks**, enabling efficient analysis of large datasets in a containerized environment.  

## **Features**  
- **Hadoop HDFS Setup** – Namenode and Datanodes for distributed storage  
- **Apache Spark Cluster** – Scalable distributed processing framework  
- **Jupyter Notebook Integration** – Interactive data exploration and analysis  
- **Containerized Deployment** – Easy setup and scalability using Docker  

## **Project Structure**  
```
/bigdata  
│── docker-compose.yml  # Defines multi-container setup  
│── Dockerfile-namenode  # Hadoop Namenode configuration  
│── Dockerfile-datanode  # Hadoop Datanode configuration  
│── Dockerfile-worker  # Spark worker node setup  
│── Dockerfile-boss  # Spark master node setup  
│── Dockerfile-notebook  # Jupyter Notebook with Spark integration  
│── p5.ipynb  # Sample Jupyter Notebook for analysis  
```  

## **Getting Started**  

### **Prerequisites**  
- Install **Docker** and **Docker Compose**  
- Clone the repository:  
  ```bash
  git clone https://github.com/PraneetiUW/bigdata.git
  cd bigdata
  ```

### **Deployment**  
To start the environment, run:  
```bash
docker-compose up -d
```
This will spin up **Hadoop HDFS, Spark, and Jupyter Notebook** containers.

### **Access Jupyter Notebook**  
Once running, open Jupyter Notebook in your browser:  
```
http://localhost:8888
```

## **Future Enhancements**  
- Automate data ingestion into HDFS  
- Add PySpark-based ETL and ML workflows  
- Implement Kubernetes for cluster orchestration  



